{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *\n",
    "from fastai.dataset import *\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from PIL import ImageDraw, ImageFont\n",
    "from matplotlib import patches, patheffects\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to Pascal VOC, we'll be using 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/sean/data/VOCdevkit/VOC2007/ImageSets'),\n",
       " PosixPath('/home/sean/data/VOCdevkit/VOC2007/pascal_train2007.json'),\n",
       " PosixPath('/home/sean/data/VOCdevkit/VOC2007/pascal_val2007.json'),\n",
       " PosixPath('/home/sean/data/VOCdevkit/VOC2007/Annotations'),\n",
       " PosixPath('/home/sean/data/VOCdevkit/VOC2007/annotations_cache'),\n",
       " PosixPath('/home/sean/data/VOCdevkit/VOC2007/pascal_test2007.json'),\n",
       " PosixPath('/home/sean/data/VOCdevkit/VOC2007/SegmentationObject'),\n",
       " PosixPath('/home/sean/data/VOCdevkit/VOC2007/SegmentationClass'),\n",
       " PosixPath('/home/sean/data/VOCdevkit/VOC2007/results'),\n",
       " PosixPath('/home/sean/data/VOCdevkit/VOC2007/JPEGImages')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = Path('/home/sean/data/VOCdevkit/VOC2007')\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['images', 'type', 'annotations', 'categories'])\n",
      "{'segmentation': [[155, 96, 155, 270, 351, 270, 351, 96]], 'area': 34104, 'iscrowd': 0, 'image_id': 12, 'bbox': [155, 96, 196, 174], 'category_id': 7, 'id': 1, 'ignore': 0} \n",
      "-----\n",
      "{'file_name': '000012.jpg', 'height': 333, 'width': 500, 'id': 12} \n",
      "-----\n",
      "[{'supercategory': 'none', 'id': 1, 'name': 'aeroplane'}, {'supercategory': 'none', 'id': 2, 'name': 'bicycle'}, {'supercategory': 'none', 'id': 3, 'name': 'bird'}, {'supercategory': 'none', 'id': 4, 'name': 'boat'}, {'supercategory': 'none', 'id': 5, 'name': 'bottle'}, {'supercategory': 'none', 'id': 6, 'name': 'bus'}, {'supercategory': 'none', 'id': 7, 'name': 'car'}, {'supercategory': 'none', 'id': 8, 'name': 'cat'}, {'supercategory': 'none', 'id': 9, 'name': 'chair'}, {'supercategory': 'none', 'id': 10, 'name': 'cow'}, {'supercategory': 'none', 'id': 11, 'name': 'diningtable'}, {'supercategory': 'none', 'id': 12, 'name': 'dog'}, {'supercategory': 'none', 'id': 13, 'name': 'horse'}, {'supercategory': 'none', 'id': 14, 'name': 'motorbike'}, {'supercategory': 'none', 'id': 15, 'name': 'person'}, {'supercategory': 'none', 'id': 16, 'name': 'pottedplant'}, {'supercategory': 'none', 'id': 17, 'name': 'sheep'}, {'supercategory': 'none', 'id': 18, 'name': 'sofa'}, {'supercategory': 'none', 'id': 19, 'name': 'train'}, {'supercategory': 'none', 'id': 20, 'name': 'tvmonitor'}]\n"
     ]
    }
   ],
   "source": [
    "train_j = json.load((PATH/'pascal_train2007.json').open())\n",
    "print(train_j.keys())\n",
    "IMAGES,ANNOTATIONS,CATEGORIES = ['images', 'annotations', 'categories']\n",
    "annotations = train_j['annotations']\n",
    "print(annotations[0], '\\n-----')\n",
    "print(train_j[IMAGES][0], '\\n-----')\n",
    "print(train_j[CATEGORIES])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seem to have some understanding of the structure of these labels, the annotations are key and then the image id is a lookup into the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lookup from category ID to class name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2name = {int(l['id']) : l['name'] for l in train_j[CATEGORIES]}\n",
    "name2id = {l['name'] : int(l['id']) for l in train_j[CATEGORIES]}\n",
    "cats = id2name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "imid2name = {int(i['id']): i['file_name'] for i in train_j[IMAGES]}\n",
    "name2imid = {i['file_name'] : int(i['id']) for i in train_j[IMAGES]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now have the training data, should probably convert bbox format to same as Jeremy, for consistancy. \n",
    "\n",
    "That is, x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_voc2np(bb):\n",
    "    return np.array([bb[1], bb[0], bb[3]+bb[1]-1, bb[2]+bb[0]-1])\n",
    "\n",
    "def bb_np2voc(bb):\n",
    "    return np.array([bb[1],bb[0],bb[3]-bb[1],bb[2]-bb[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get an array of dicts for training data\n",
    "\n",
    "We want each dict to have 'image_id', 'bbox', 'category_id' and 'ingore' (pretty sure jeremy does something for ignore). These ignore flags represent harder detections where multiple instances are overlapping or the object is heavily occluded, i am pretty sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevent_data(split_j, des_keys=['image_id', 'bbox', 'category_id']):\n",
    "    # we do not want the ignores\n",
    "    return [{key : datum[key] for key in des_keys} for datum in split_j[ANNOTATIONS] if not datum['ignore']]\n",
    "\n",
    "def load_json_relevent(fname):\n",
    "    jfile = json.load((PATH/fname).open())\n",
    "    return get_relevent_data(jfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions create a list of the training, which is not what Jeremy does, he creates a dict with image_id as keys and bb with caterogy at items. So I will do the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dict(split_j):\n",
    "    dat = collections.defaultdict(lambda:[])\n",
    "    for l in split_j[ANNOTATIONS]:\n",
    "        if not l['ignore']:\n",
    "            bbox = bb_voc2np(l['bbox'])\n",
    "            dat[l['image_id']].append((bbox, l['category_id']))\n",
    "    return dat\n",
    "\n",
    "train = get_data_dict(train_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use \"imid2name\" to load one of the images using FastAI's open image function\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
